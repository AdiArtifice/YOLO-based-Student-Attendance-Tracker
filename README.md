# YOLO Attendance Tracker

Automated student attendance system powered by a Roboflow workflow. A FastAPI backend accepts classroom photos, forwards them to the workflow (local or hosted inference), and returns structured attendance plus the workflow‚Äôs own visualization image. A modern frontend previews the annotated image‚Äîcomplete with roll numbers, ‚ÄúUnknown‚Äù tags, and color-coded boxes exactly as configured in Roboflow.

---

## üì¶ Project Overview

- **Purpose:** Detect students in classroom imagery, tag them with roll numbers or ‚ÄúUnknown,‚Äù and produce attendance records automatically.
- **Core Components:**
	- **Workflow & Inference:** Roboflow workflow executed via the Inference server/hosted API.
	- **Backend (`app/`):** FastAPI service that uploads images, calls the workflow, parses attendance, and streams back visualization data.
	- **Frontend (`frontend/`):** Single-page experience for uploading images and previewing annotated results.
	- **CLI (`test-roboflow.py`):** Utility script for batch processing or debugging workflows from the terminal.
- **Visualization:** The returned annotated preview is the exact visualization generated by Roboflow visualization blocks‚Äîno Python post-processing required.

---

## üóÇÔ∏è Directory Structure

```
.
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ attendance.py        # Workflow client, prediction parsing, attendance builder, visualization extractor
‚îÇ  ‚îî‚îÄ main.py              # FastAPI routes, file handling, response shaping
‚îú‚îÄ frontend/
‚îÇ  ‚îî‚îÄ index.html           # Upload UI, attendance table, annotated image preview
‚îú‚îÄ images/                 # (Optional) Sample input images you provide
‚îú‚îÄ outputs/                # CLI script outputs (CSV, crops, JSON, annotated images)
‚îú‚îÄ test-roboflow.py        # CLI runner for images, folders, or webcam sources
‚îú‚îÄ requirements.txt        # Python dependencies (FastAPI, inference SDK, etc.)
‚îú‚îÄ .env                    # Environment variables (never commit real secrets)
‚îú‚îÄ .gitignore
‚îî‚îÄ README.md               # You are here
```

---

## üõ†Ô∏è Setup Instructions

### Prerequisites

- Windows 10/11 (WSL2 recommended for Docker performance)
- [Docker Desktop](https://www.docker.com/products/docker-desktop) running (green whale icon)
- Python 3.11 (via `py -3.11` or official installer)
- Roboflow account with a workflow configured for student detection + visualization blocks

### 1. Clone the repository

```powershell
git clone https://github.com/AdiArtifice/YOLO-based-Student-Attendance-Tracker.git
cd YOLO-based-Student-Attendance-Tracker\main
```

### 2. Create & activate a virtual environment (Python 3.11)

```powershell
python -m venv .venv311
.\.venv311\Scripts\Activate.ps1
python -V   # expect 3.11.x
```

If that still shows Python 3.13, force the version:

```powershell
py -3.11 -m venv .venv311
.\.venv311\Scripts\Activate.ps1
```

### 3. Install dependencies

```powershell
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Configure environment variables (`.env`)

Create `.env` in the project root (the `main` folder) with your Roboflow details. Never commit real keys.

```powershell
@'
ROBOFLOW_API_KEY=REPLACE_WITH_YOUR_KEY
ROBOFLOW_API_URL=http://localhost:9001
ROBOFLOW_WORKSPACE=your-workspace-slug
ROBOFLOW_WORKFLOW=your-workflow-id
@' | Out-File -FilePath .env -Encoding utf8 -NoNewline
```

- `ROBOFLOW_API_URL` can point to either the local inference server (`http://localhost:9001`) or the hosted endpoint.
- Ensure the workflow ends with visualization blocks so the API response includes rendered overlays.

### 5. Start Docker Desktop & verify

```powershell
docker version
```

Both *Client* and *Server* sections must show up.

---

## ‚ñ∂Ô∏è Running the Project

### 1. Launch the Roboflow Inference server (local mode)

```powershell
inference server start --port 9001 --roboflow-api-key $env:ROBOFLOW_API_KEY
```

Watch for `Uvicorn running on http://0.0.0.0:9001`.

Health endpoint (may return 404, but confirms the server responds):

```powershell
curl http://localhost:9001/health
```

> **Hosted fallback:** omit this step and set `ROBOFLOW_API_URL` to the hosted API URL if you don‚Äôt want Docker.

### 2. Start the FastAPI backend + frontend bundle

```powershell
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Open <http://localhost:8000>. The SPA lives inside `app.main`, so you don‚Äôt need a separate static file server.

### 3. Upload an image via the browser

- Click ‚ÄúRun Attendance,‚Äù choose a classroom photo, and wait for the response.
- Each roll number in the attendance table is clickable‚Äîopen it to view the face crop returned by the workflow‚Äôs dynamic crop block for rapid identity checks.
- The backend saves the file to a temporary path, calls `run_workflow`, and immediately deletes the temporary file once inference is finished.
- The response includes:
	- `attendance`: list of roll numbers with `status` and `confidence`.
	- `detections`: raw prediction dictionaries for debugging.
	- `annotated_image`: data URI or URL sourced directly from the workflow‚Äôs visualization block.

### 4. Command-line workflow runner (`test-roboflow.py`)

Single image:

```powershell
python test-roboflow.py --image .\images\sample.jpg --api-url auto --save-annotated --json
```

Entire folder:

```powershell
python test-roboflow.py --folder .\images --save-annotated --save-crops --json
```

Webcam (device 0):

```powershell
python test-roboflow.py --webcam 0
```

Flags:
- `--api-url auto` chooses local server when available, otherwise falls back to hosted API.
- `--save-annotated`, `--save-crops`, `--json` persist extra artifacts under `outputs/`.

---

## üì§ Expected Outputs

- **Attendance JSON:** Returned by the API/CLI. Each entry has `roll_number`, `status` (`present`), and normalized `confidence`.
- **Detections JSON:** Optional raw predictions for debugging.
- **Visualization Image:** The Roboflow-generated overlay (data URI or URL). Preserves roll numbers, custom colors, and any styling configured in the visualization block.
- **CSV Reports:** When using the CLI with `--json` or default modes, per-image CSVs save to `outputs/`.
- **Crops:** The web API returns base64 crops inline (`attendance[*].crop_image`) for a stateless experience. The CLI can still write disk files with `--save-crops` when needed.
- **Temporary Files:** The FastAPI app keeps uploads only for the duration of inference; nothing persists on disk after the response.

---

## üß© Troubleshooting & Tips

| Problem | Likely Cause | Fix |
|---------|--------------|-----|
| `inference` command not found | `inference-cli` not installed into current venv | Re-run `pip install -r requirements.txt` inside the active `.venv311`. |
| `No matching distribution for inference-cli` | Using Python 3.13+ | Switch to Python 3.11 virtual environment. |
| `Error connecting to Docker daemon` | Docker Desktop not running or WSL integration disabled | Start Docker Desktop, ensure WSL2 backend is enabled, retry. |
| API response missing `annotated_image` | Workflow lacks a visualization block or it‚Äôs not last | Reorder your workflow so visualization blocks run last (see Roboflow docs). |
| Attendance rows disabled / ‚ÄúNo face crops were returned‚Äù | Dynamic Crop block not returning images | Place the Dynamic Crop block directly after detection, filter the exact class (`student-faces`), enable ‚ÄúReturn cropped images in response,‚Äù and re-run inference. Check the API `debug` payload for crop counts. |
| FastAPI returns 500 with ‚ÄúInference failed‚Äù | Bad API key, incorrect workspace/workflow, or server offline | Confirm `.env` values, verify the inference server logs, and retry. |
| Frontend shows HTTP 405/404 | Uploading from a different origin | Visit <http://localhost:8000> directly; the SPA expects the API on the same host/port. |

Log output from `uvicorn` and the inference server is extremely helpful‚Äîkeep terminals open while testing.

### Dynamic Crop debugging checklist

1. **Verify class names** ‚Äì Run `python test-roboflow.py --image <file> --json` and inspect the generated `_raw.json` or the API response `debug.detection_classes`. The crop filter must match the detected class exactly (case-sensitive `student-faces`).
2. **Block order** ‚Äì In Roboflow Studio, wire blocks as `Input ‚Üí Detection ‚Üí Dynamic Crop ‚Üí Visualization/Output`. The crop block must consume detection outputs.
3. **Enable responses** ‚Äì Tick ‚ÄúReturn cropped images in response‚Äù inside the Dynamic Crop block so base64 payloads are emitted.
4. **Threshold sanity** ‚Äì If confidence (0.4) or IoU (0.75) are too strict, temporarily lower them and re-test. Monitor `debug.crop_sources` in the API response to see how many rows received crops.
5. **Inspect raw data** ‚Äì Use the CLI `*_raw.json` or add `--json` to surface the entire workflow response. Confirm `steps.dynamic_crop.crops` (or equivalent) exists and each entry exposes `type: base64` and a `value` string.
6. **Review API debug stats** ‚Äì Every `/api/upload` response includes `debug.crop_stats` (counts of scanned crops, base64 payloads, and successful conversions) plus `debug.crop_map_counts`. Non-zero values confirm the backend parsed your crops.
7. **Refresh versions** ‚Äì After editing the workflow, publish the new version and restart the inference server/Uvicorn so the latest graph is loaded (avoid stale caches).
8. **Fallback crops** ‚Äì The backend automatically matches crops by prediction ID, label (case-insensitive), and finally a fallback queue; the `debug.crop_map_counts` field helps confirm how many crops were supplied via each path.
9. **Manual safety net** ‚Äì If the workflow returns no dynamic crops, the API now synthesizes face crops directly from the detection bounding boxes so the modal still works; these appear in `debug.crop_stats.manual_bbox_generated`.

---

## üîß Customization & Extension

- **Swap Workflows/Models:** Update `ROBOFLOW_WORKSPACE` and `ROBOFLOW_WORKFLOW`, then ensure the workflow outputs roll numbers or metadata your attendance logic expects.
- **Adjust Visualization:** Inside Roboflow, edit visualization blocks to tweak colors, label templates (`{{ metadata.roll_number }}`), or add label stacking. The backend simply forwards whatever the workflow returns.
- **Map to Student Directory:** Enhance `build_attendance` in `app/attendance.py` to reconcile roll numbers, handle ‚ÄúUnknown,‚Äù or sync results into a database/CSV export.
- **Integrate with LMS/ERP:** Create new FastAPI routes for exporting attendance (CSV, Google Sheets, REST hooks). You can reuse the existing response payloads.
- **Batch Jobs:** Schedule `test-roboflow.py` for nightly folder scans, or wrap it with a task runner like Airflow/Celery.

---

## ü§ù Contributing

1. Fork the repository and create a feature branch.
2. Make your changes with clear commits.
3. Run relevant tests or smoke checks (`python -m compileall app`, sample inference).
4. Submit a pull request describing the change and how to reproduce results.

Bug reports and feature suggestions are welcome via issues or discussions.

---

## üìÑ License

This project is intended for internal and educational use. Adapt or extend it responsibly‚Äîespecially when handling student data.
